# LLM Hallucinations

## What are Hallucinations?

- When LLMs generate false or made-up information
- Can appear convincing but be completely incorrect
- A significant challenge in AI safety and reliability

## Common Types of Hallucinations

- Fabricating facts, statistics, or references
- Creating non-existent citations or sources
- Inventing technical details or procedures
- Mixing up or combining unrelated information

---
