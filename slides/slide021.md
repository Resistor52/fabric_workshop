# Mitigating Hallucinations

- Use lower temperature settings (0.0-0.3)
- Always verify critical information
- Cross-reference with trusted sources
- Be especially careful with:
  - Technical specifications
  - Historical dates and facts
  - Citations and references
  - Security-related information

## Best Practices

- Treat LLM outputs as suggestions, not facts
- Implement human verification for critical tasks
- Use system prompts that emphasize accuracy
- Consider using multiple LLMs for cross-validation

---